{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the YouTube API \n",
    "This file automatically downloads transcripts and video information using the YouTube API. For help see: <br> \n",
    "https://developers.google.com/youtube/v3/ <br>\n",
    "https://github.com/spnichol/youtube_tutorial/blob/master/youtube_videos.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search YT for videos\n",
    "This section contains two functions, but I will only use one. <br> The first one runs a search and returns 50 results. I'm going to use that to obtain safe and unrestricted content. <br> The second one uses the video resource https://developers.google.com/youtube/v3/docs/videos to load extra information about each video (by id). I won't use that because there's a lot of missing data for the parameter's I'm interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to query the API\n",
    "from apiclient.discovery import build\n",
    "\n",
    "# Set key and enable YouTube Data API for your project.\n",
    "DEVELOPER_KEY = \"AIzaSyBP5sx70WtDUst-hL41i2fBQOZXXOssjDI\" # your key here\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "# this function searches YT for videos only\n",
    "def youtube_search(q, token=None, safe = 'none'): #can specify 'strict' to build validation set\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)\n",
    "    \n",
    "    search_response = youtube.search().list(\n",
    "        q=q,\n",
    "        type='video',\n",
    "        pageToken=token,\n",
    "        order='relevance',\n",
    "        part='id,snippet',\n",
    "        maxResults=50,\n",
    "        relevanceLanguage='en',\n",
    "        safeSearch=safe\n",
    "    ).execute()\n",
    "    \n",
    "    videos = [] \n",
    "    for search_result in search_response.get(\"items\", []): \n",
    "        videos.append(search_result)\n",
    "\n",
    "    try:\n",
    "        nexttok = search_response[\"nextPageToken\"]\n",
    "        return(nexttok, videos)\n",
    "    except Exception as e:\n",
    "        nexttok = \"last_page\"\n",
    "        return(nexttok, videos)\n",
    "    \n",
    "# this function gets extra information about a video    \n",
    "def geo_query(video_id):\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "                    developerKey=DEVELOPER_KEY)\n",
    "\n",
    "    video_response = youtube.videos().list(\n",
    "        id=video_id,\n",
    "        part='contentDetails, statistics'\n",
    "    ).execute()\n",
    "\n",
    "    return video_response\n",
    "\n",
    "# make table: extract json, ID for urls, title, description, thumbnail\n",
    "def make_results_table(json_results):\n",
    "    yt_urls = []\n",
    "    titles = []\n",
    "    description = []\n",
    "    channel = []\n",
    "    thumbnail = []\n",
    "    \n",
    "    for video in json_results:\n",
    "        yt_urls.append(video['id']['videoId'])\n",
    "        titles.append(video['snippet']['title'])\n",
    "        description.append(video['snippet']['description'])\n",
    "        channel.append(video['snippet']['channelTitle'])\n",
    "        thumbnail.append(video['snippet']['thumbnails']['default']['url'])\n",
    "    \n",
    "    video_table = pd.DataFrame({\"url\":yt_urls, \"title\":titles, \"description\":description, \"channel\":channel, \"thumbnail\":thumbnail})\n",
    "    return video_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run YT search\n",
    "search_results = youtube_search(\"disney\")#\"{}\".format(search)) # can specify safe = 'strict' for a safe search\n",
    "search_token = search_results[0]\n",
    "search_json = search_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>channel</th>\n",
       "      <th>thumbnail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xHpH11hiWfg</td>\n",
       "      <td>RALPH BREAKS THE INTERNET: Wreck-it Ralph 2 Tr...</td>\n",
       "      <td>Ralph's back! Check out our brand trailer for ...</td>\n",
       "      <td>Disney UK</td>\n",
       "      <td>https://i.ytimg.com/vi/xHpH11hiWfg/default.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p4D19K8s-lA</td>\n",
       "      <td>10 Theories That Make Disney Movies So Much Da...</td>\n",
       "      <td>Beware, some of these fan theories just might ...</td>\n",
       "      <td>Screen Rant</td>\n",
       "      <td>https://i.ytimg.com/vi/p4D19K8s-lA/default.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_30jPKzWdN0</td>\n",
       "      <td>Why Are There No Mosquitoes at Disney World?</td>\n",
       "      <td>Walt Disney World is smack dab in the middle o...</td>\n",
       "      <td>Rob Plays</td>\n",
       "      <td>https://i.ytimg.com/vi/_30jPKzWdN0/default.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BJzMhLhc070</td>\n",
       "      <td>Going Batty / Scare B&amp;B | Full Episode | Vampi...</td>\n",
       "      <td>Vampirina and her family move to Pennsylvania ...</td>\n",
       "      <td>disneyjunior</td>\n",
       "      <td>https://i.ytimg.com/vi/BJzMhLhc070/default.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IPSdb0JPnK8</td>\n",
       "      <td>A Disney Springs Update From 400 Feet Up, Chec...</td>\n",
       "      <td>In today's vlog we head over to Disney Springs...</td>\n",
       "      <td>TheTimTracker</td>\n",
       "      <td>https://i.ytimg.com/vi/IPSdb0JPnK8/default.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           url                                              title  \\\n",
       "0  xHpH11hiWfg  RALPH BREAKS THE INTERNET: Wreck-it Ralph 2 Tr...   \n",
       "1  p4D19K8s-lA  10 Theories That Make Disney Movies So Much Da...   \n",
       "2  _30jPKzWdN0       Why Are There No Mosquitoes at Disney World?   \n",
       "3  BJzMhLhc070  Going Batty / Scare B&B | Full Episode | Vampi...   \n",
       "4  IPSdb0JPnK8  A Disney Springs Update From 400 Feet Up, Chec...   \n",
       "\n",
       "                                         description        channel  \\\n",
       "0  Ralph's back! Check out our brand trailer for ...      Disney UK   \n",
       "1  Beware, some of these fan theories just might ...    Screen Rant   \n",
       "2  Walt Disney World is smack dab in the middle o...      Rob Plays   \n",
       "3  Vampirina and her family move to Pennsylvania ...   disneyjunior   \n",
       "4  In today's vlog we head over to Disney Springs...  TheTimTracker   \n",
       "\n",
       "                                        thumbnail  \n",
       "0  https://i.ytimg.com/vi/xHpH11hiWfg/default.jpg  \n",
       "1  https://i.ytimg.com/vi/p4D19K8s-lA/default.jpg  \n",
       "2  https://i.ytimg.com/vi/_30jPKzWdN0/default.jpg  \n",
       "3  https://i.ytimg.com/vi/BJzMhLhc070/default.jpg  \n",
       "4  https://i.ytimg.com/vi/IPSdb0JPnK8/default.jpg  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table = make_results_table(search_json)\n",
    "results_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "kid_searches = ['peppa pig','Frozen','PAW patrol','oggy','powerpuff girls','mickey mouse','minnie mouse','dora','doraemon','wonder woman',\n",
    "           'star wars','batman','superman','lego','power rangers','bugs bunny','elsa','baby einstein','spiderman','scooby doo',\n",
    "           'alphabet','animals','winnie the pooh','tom and jerry','disney','sesame street','school','ufo','little pony','sponge bob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_list=[]\n",
    "unsafe_list=[]\n",
    "\n",
    "for each in kid_searches:\n",
    "    safe_results = youtube_search(each, safe='strict') \n",
    "    safe_json = safe_results[1]\n",
    "    safe_results_table = make_results_table(safe_json)\n",
    "    safe_results_table['query']=each\n",
    "    safe_list.append(safe_results_table)\n",
    "    unsafe_results = youtube_search(each) \n",
    "    unsafe_json = unsafe_results[1]\n",
    "    unsafe_results_table = make_results_table(unsafe_json)\n",
    "    unsafe_results_table['query']=each\n",
    "    unsafe_list.append(unsafe_results_table)\n",
    "\n",
    "safe_table = pd.concat(safe_list)\n",
    "unsafe_table = pd.concat(unsafe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 6) (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "print(safe_table.shape, unsafe_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>channel</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UhNX5NdJCs4</td>\n",
       "      <td>Peppa Pig Full Episodes | LIVE Peppa Pig 2018 ...</td>\n",
       "      <td>Peppa Pig Full Episodes | LIVE Peppa Pig 2017 ...</td>\n",
       "      <td>Peppa Pig - Official Channel</td>\n",
       "      <td>https://i.ytimg.com/vi/UhNX5NdJCs4/default_liv...</td>\n",
       "      <td>peppa pig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HRcB1WOQZIo</td>\n",
       "      <td>Peppa Pig English Episodes | Parachute Jump | ...</td>\n",
       "      <td>Peppa Pig English Episodes | Parachute Jump | ...</td>\n",
       "      <td>Peppa Pig - Official Channel</td>\n",
       "      <td>https://i.ytimg.com/vi/HRcB1WOQZIo/default.jpg</td>\n",
       "      <td>peppa pig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W9JbqgenXUE</td>\n",
       "      <td>Peppa Pig English Episodes in 4K | Scooters! |...</td>\n",
       "      <td>We come back to discover brand new clips from ...</td>\n",
       "      <td>Peppa Pig - Official Channel</td>\n",
       "      <td>https://i.ytimg.com/vi/W9JbqgenXUE/default.jpg</td>\n",
       "      <td>peppa pig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P18xl3yDMX0</td>\n",
       "      <td>Peppa Pig English Episodes | Robbie and Rosie ...</td>\n",
       "      <td>We have yet more new characters joining the wo...</td>\n",
       "      <td>Peppa Pig - Official Channel</td>\n",
       "      <td>https://i.ytimg.com/vi/P18xl3yDMX0/default.jpg</td>\n",
       "      <td>peppa pig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s7IL4pERfr0</td>\n",
       "      <td>Peppa Pig English Episodes | Pottery with Pepp...</td>\n",
       "      <td>Subscribe for more videos: http://bit.ly/Peppa...</td>\n",
       "      <td>Peppa Pig - Official Channel</td>\n",
       "      <td>https://i.ytimg.com/vi/s7IL4pERfr0/default.jpg</td>\n",
       "      <td>peppa pig</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           url                                              title  \\\n",
       "0  UhNX5NdJCs4  Peppa Pig Full Episodes | LIVE Peppa Pig 2018 ...   \n",
       "1  HRcB1WOQZIo  Peppa Pig English Episodes | Parachute Jump | ...   \n",
       "2  W9JbqgenXUE  Peppa Pig English Episodes in 4K | Scooters! |...   \n",
       "3  P18xl3yDMX0  Peppa Pig English Episodes | Robbie and Rosie ...   \n",
       "4  s7IL4pERfr0  Peppa Pig English Episodes | Pottery with Pepp...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Peppa Pig Full Episodes | LIVE Peppa Pig 2017 ...   \n",
       "1  Peppa Pig English Episodes | Parachute Jump | ...   \n",
       "2  We come back to discover brand new clips from ...   \n",
       "3  We have yet more new characters joining the wo...   \n",
       "4  Subscribe for more videos: http://bit.ly/Peppa...   \n",
       "\n",
       "                        channel  \\\n",
       "0  Peppa Pig - Official Channel   \n",
       "1  Peppa Pig - Official Channel   \n",
       "2  Peppa Pig - Official Channel   \n",
       "3  Peppa Pig - Official Channel   \n",
       "4  Peppa Pig - Official Channel   \n",
       "\n",
       "                                           thumbnail      query  \n",
       "0  https://i.ytimg.com/vi/UhNX5NdJCs4/default_liv...  peppa pig  \n",
       "1     https://i.ytimg.com/vi/HRcB1WOQZIo/default.jpg  peppa pig  \n",
       "2     https://i.ytimg.com/vi/W9JbqgenXUE/default.jpg  peppa pig  \n",
       "3     https://i.ytimg.com/vi/P18xl3yDMX0/default.jpg  peppa pig  \n",
       "4     https://i.ytimg.com/vi/s7IL4pERfr0/default.jpg  peppa pig  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsafe_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 6)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove any safe content from the unsafe table \n",
    "notsafe = unsafe_table.loc[~unsafe_table['url'].isin(safe_table['url'])]\n",
    "notsafe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn urls into transcripts\n",
    "from webvtt import WebVTT\n",
    "\n",
    "# https://stackoverflow.com/questions/48125300/cant-scrape-youtube-videos-closed-captions\n",
    "#I'll need to autopopulate the output filenames with %\n",
    "def download_subs(video_url, lang=\"en\"):\n",
    "    cmd = [\n",
    "        \"youtube-dl\",\n",
    "        \"--skip-download\",\n",
    "        \"--write-auto-sub\",\n",
    "        \"--sub-format vtt\",\n",
    "        \"--sub-lang\",\n",
    "        lang,\n",
    "        video_url,\n",
    "        \"--output test.vtt\"\n",
    "    ]\n",
    "    \n",
    "    command_string = \" \".join(cmd)\n",
    "    print(command_string)\n",
    "    \n",
    "    os.system(command_string)\n",
    "    # https://pypi.org/project/webvtt-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
